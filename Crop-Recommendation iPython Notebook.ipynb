{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing my toolkits here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    N   P   K  temperature   humidity        ph    rainfall label\n",
      "0  90  42  43    20.879744  82.002744  6.502985  202.935536  rice\n",
      "1  85  58  41    21.770462  80.319644  7.038096  226.655537  rice\n",
      "2  60  55  44    23.004459  82.320763  7.840207  263.964248  rice\n",
      "3  74  35  40    26.491096  80.158363  6.980401  242.864034  rice\n",
      "4  78  42  42    20.130175  81.604873  7.628473  262.717340  rice\n",
      "Shape of Dataset:  (2200, 8)\n"
     ]
    }
   ],
   "source": [
    "initial_d = pd.read_csv('Crop_recommendation.csv')\n",
    "print(initial_d.head(5))\n",
    "print(\"Shape of Dataset: \", initial_d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All cool till now :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating Labels from Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:-\n",
      "0    rice\n",
      "1    rice\n",
      "2    rice\n",
      "3    rice\n",
      "4    rice\n",
      "Name: label, dtype: object\n",
      "Features:-\n",
      "    N   P   K  temperature   humidity        ph    rainfall\n",
      "0  90  42  43    20.879744  82.002744  6.502985  202.935536\n",
      "1  85  58  41    21.770462  80.319644  7.038096  226.655537\n",
      "2  60  55  44    23.004459  82.320763  7.840207  263.964248\n",
      "3  74  35  40    26.491096  80.158363  6.980401  242.864034\n",
      "4  78  42  42    20.130175  81.604873  7.628473  262.717340\n",
      "(2200, 7)\n"
     ]
    }
   ],
   "source": [
    "init_label= initial_d['label']\n",
    "init_features = initial_d.drop(\"label\",axis=1)\n",
    "print(\"Labels:-\")\n",
    "print(init_label.head(5))\n",
    "print(\"Features:-\")\n",
    "print(init_features.head(5))\n",
    "print(init_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6\n",
      "0  1.068797 -0.344551 -0.101688 -0.935587  0.472666  0.043302  1.810361\n",
      "1  0.933329  0.140616 -0.141185 -0.759646  0.397051  0.734873  2.242058\n",
      "2  0.255986  0.049647 -0.081939 -0.515898  0.486954  1.771510  2.921066\n",
      "3  0.635298 -0.556811 -0.160933  0.172807  0.389805  0.660308  2.537048\n",
      "4  0.743673 -0.344551 -0.121436 -1.083647  0.454792  1.497868  2.898373\n"
     ]
    }
   ],
   "source": [
    "#Standardizing the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardized_features = StandardScaler().fit_transform(init_features) # Returns np array\n",
    "standardized_features = pd.DataFrame(standardized_features) #Convert back np array to Panda DataFrame\n",
    "print(standardized_features.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All good for first model-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "Train Data Shape\n",
      "(1760, 7)\n",
      "(1760,)\n",
      "===========================\n",
      "Test Data Shape\n",
      "(440, 7)\n",
      "(440,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#We have 2200 Points; \n",
    "#We split it such that we get 20% of the points(440) points in Test Dataset\n",
    "Xtrain0, Xtest, Ytrain0, Ytest= model_selection.train_test_split(init_features, init_label, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"==========================\")\n",
    "print(\"Train Data Shape\")\n",
    "print(Xtrain0.shape)\n",
    "print(Ytrain0.shape)\n",
    "print(\"===========================\")\n",
    "print(\"Test Data Shape\")\n",
    "print(Xtest.shape)\n",
    "print(Ytest.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are using Cross Validation for determining right value of K(Hyperparameter )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cross Validation Score for k = 1 is \n",
      "0.9823863636363637\n",
      "\n",
      " Cross Validation Score for k = 3 is \n",
      "0.9801136363636364\n",
      "\n",
      " Cross Validation Score for k = 5 is \n",
      "0.9806818181818182\n",
      "\n",
      " Cross Validation Score for k = 7 is \n",
      "0.9806818181818182\n",
      "\n",
      " Cross Validation Score for k = 9 is \n",
      "0.9772727272727273\n",
      "\n",
      " Cross Validation Score for k = 11 is \n",
      "0.975\n",
      "\n",
      " Cross Validation Score for k = 13 is \n",
      "0.975\n",
      "\n",
      " Cross Validation Score for k = 15 is \n",
      "0.9715909090909092\n",
      "\n",
      " Cross Validation Score for k = 17 is \n",
      "0.9698863636363637\n",
      "\n",
      " Cross Validation Score for k = 19 is \n",
      "0.96875\n",
      "\n",
      " Cross Validation Score for k = 21 is \n",
      "0.9670454545454545\n",
      "\n",
      " Cross Validation Score for k = 23 is \n",
      "0.965909090909091\n",
      "\n",
      " Cross Validation Score for k = 25 is \n",
      "0.9659090909090908\n",
      "\n",
      " Cross Validation Score for k = 27 is \n",
      "0.9613636363636363\n",
      "\n",
      " Cross Validation Score for k = 29 is \n",
      "0.9607954545454545\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 30, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    score = cross_val_score(knn, Xtrain0.to_numpy(), Ytrain0, cv=5, scoring='accuracy')\n",
    "    print('\\n Cross Validation Score for k = %d is ' % (i))\n",
    "    print(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K=5 gives us best score on cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score for this Model is= 97.045455%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       apple       1.00      1.00      1.00        23\n",
      "      banana       1.00      1.00      1.00        21\n",
      "   blackgram       0.95      1.00      0.98        20\n",
      "    chickpea       1.00      1.00      1.00        26\n",
      "     coconut       1.00      1.00      1.00        27\n",
      "      coffee       1.00      0.94      0.97        17\n",
      "      cotton       0.94      1.00      0.97        17\n",
      "      grapes       1.00      1.00      1.00        14\n",
      "        jute       0.76      0.96      0.85        23\n",
      " kidneybeans       0.95      1.00      0.98        20\n",
      "      lentil       0.85      1.00      0.92        11\n",
      "       maize       1.00      0.95      0.98        21\n",
      "       mango       1.00      1.00      1.00        19\n",
      "   mothbeans       1.00      0.92      0.96        24\n",
      "    mungbean       1.00      1.00      1.00        19\n",
      "   muskmelon       1.00      1.00      1.00        17\n",
      "      orange       1.00      1.00      1.00        14\n",
      "      papaya       1.00      1.00      1.00        23\n",
      "  pigeonpeas       1.00      0.91      0.95        23\n",
      " pomegranate       1.00      1.00      1.00        23\n",
      "        rice       0.93      0.68      0.79        19\n",
      "  watermelon       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.97       440\n",
      "   macro avg       0.97      0.97      0.97       440\n",
      "weighted avg       0.97      0.97      0.97       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(Xtrain0.to_numpy(), Ytrain0)\n",
    "pred = knn.predict(Xtest.to_numpy())\n",
    "acc = accuracy_score(Ytest, pred, normalize=True) * float(100)\n",
    "print('Test Accuracy Score for this Model is= %f%%' % (acc))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Ytest, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold CV score for Naive Bayes:,  0.9954545454545455\n",
      "\n",
      "Gaussian Naive Bayes's Accuracy aginst Test Dataset: 0.9954545454545455\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       apple       1.00      1.00      1.00        23\n",
      "      banana       1.00      1.00      1.00        21\n",
      "   blackgram       1.00      1.00      1.00        20\n",
      "    chickpea       1.00      1.00      1.00        26\n",
      "     coconut       1.00      1.00      1.00        27\n",
      "      coffee       1.00      1.00      1.00        17\n",
      "      cotton       1.00      1.00      1.00        17\n",
      "      grapes       1.00      1.00      1.00        14\n",
      "        jute       0.92      1.00      0.96        23\n",
      " kidneybeans       1.00      1.00      1.00        20\n",
      "      lentil       1.00      1.00      1.00        11\n",
      "       maize       1.00      1.00      1.00        21\n",
      "       mango       1.00      1.00      1.00        19\n",
      "   mothbeans       1.00      1.00      1.00        24\n",
      "    mungbean       1.00      1.00      1.00        19\n",
      "   muskmelon       1.00      1.00      1.00        17\n",
      "      orange       1.00      1.00      1.00        14\n",
      "      papaya       1.00      1.00      1.00        23\n",
      "  pigeonpeas       1.00      1.00      1.00        23\n",
      " pomegranate       1.00      1.00      1.00        23\n",
      "        rice       1.00      0.89      0.94        19\n",
      "  watermelon       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           1.00       440\n",
      "   macro avg       1.00      1.00      1.00       440\n",
      "weighted avg       1.00      1.00      1.00       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "GNBclf = GaussianNB()\n",
    "GNBclf.fit(Xtrain0, Ytrain0)\n",
    "YPred = GNBclf.predict(Xtest)\n",
    "score = cross_val_score(GNBclf, Xtrain0, Ytrain0, cv=5, scoring='accuracy')\n",
    "avgCVScore=score.mean()\n",
    "print(\"5-fold CV score for Naive Bayes:, \", avgCVScore)\n",
    "\n",
    "acc = accuracy_score(Ytest, YPred)\n",
    "print(\"\\nGaussian Naive Bayes's Accuracy aginst Test Dataset:\", acc)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\n\")\n",
    "print(classification_report(Ytest, YPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes has better accuracy than KNN. Hence we will chose Gaussian Naive Bayes for our recommendation system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our model on pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GNBClassifier.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=GaussianNB()\n",
    "clf.fit(init_features, init_label)\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "dump(clf, 'GNBClassifier.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a74da8edcc7f578a5fe4b10d4b546cffe6bfce965c94e0e36b2a90b879c0921e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
